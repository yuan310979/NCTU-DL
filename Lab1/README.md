Lab1. back-propagation
=====================
## Introduction
In this lab, we’ll construct a simple neural network with forward pass and back-propagation to update weight in each layer. Try to fit our model to some easy dataset.<br>
This model is designed by torch-like design pattern. Separate each element of neural network by module. And construct desired model with our design, like what we mostly done on PyTorch.<br>
Training data and testing data are created by linear function and xor function. Final result and further discussion will be presented on last part of this report.<br>

## Experiment setups
### Architeture
![](https://i.imgur.com/srMRUmE.png)

|dir|description|
|---|------------|
|**nn** | neural network related module|
|**utils** | helper module (generate data, plotting ……)| 

Simple neural network can be divided into some neurons.

And each neuron will contain four important part, which is input, weight, activation function, output.

![](https://i.imgur.com/fvuJnpQ.png)

Each neuron can be presented as the following formula.

![](https://i.imgur.com/RSpJSGL.png)

#### Activation Function
I use sigmoid function as my neuron’s activation, which may be presented as a curve like ‘s’ shape to represent the activation scale of each neuron output.<br>
![](https://i.imgur.com/KH2pjMI.png)

#### Neural Network
This model is composed with three neurons, which input feature and output feature is (2,3), (3,3), (3,1) respectively. Each neuron output will be activated by sigmoid function.

In the end, I use cross entropy loss to train our model. The reason about that will be discussed in the last part of this report. <br>

#### Back Propagation
During training process, we’ll need to update weight of each neuron. Intuitively, we can calculate the gradient of loss function with respect to each layer’s weight matrix.<br>
The following part will show the backpropagation process of each step.

Assume that we have L layers. J represent loss function. Z is the output value without passing through activation function. X is the input of each layers. a is the activation output of each layer.  Z, X and a are shown in matrix form.


Do partial derivative on the weight of output layer. <br><br>
![](https://i.imgur.com/VGsebU8.png) <br><br>
When delta can represented as follow. <br><br>
![](https://i.imgur.com/sk7WISN.png) <br><br>
Then we continue to the L-1 layer. <br><br>
![](https://i.imgur.com/mGSDwD3.png) <br><br>
When delta can represent as follow. <br><br>
![](https://i.imgur.com/hKCgFuT.png) <br><br>
Therefore, we can generalize this equation with recurrence fashion, which propagate “error” from the last layer to the beginning of the model layer.<br><br>
![](https://i.imgur.com/0aCMwcb.png)<br>
![](https://i.imgur.com/RbDR2wY.png)<br>

According to the recurrence equation, we can easily implement our neural network backpropagation, to back propagate our error term and then update weight of each layer.

## Results
The following part will be the testing result of some artificial data point, generated by linear function and XOR-like function.<br>
Each data points will be trained on our designed model with 100,000 epochs. And we’ll print out total loss every 5,000 epochs and the compared graph including ground truth distribution and predicted distribution.<br>
Learning rate of both dataset are set to 1e-3. <br>

### Linear
Linear function generate data point based on uniform distribution from 0 to 1. Then compared x[0] and x[1], if x[0] is greater than x[1], set label to 1. Labels of other points which x[0] is less than x[1] will be set to 0.<br>
![](https://i.imgur.com/MsJQiDx.png)
![](https://i.imgur.com/b1bCY2l.png)
![](https://i.imgur.com/C8bWdbt.png)

### XOR
XOR generator generate two groups of data points with diagonal shape. Upper left corner to bottom right corner data points are labeled as 1, while the others will be labeled as 0.<br>
![](https://i.imgur.com/OdU1VgA.png)
![](https://i.imgur.com/FyXSHmf.png)
![](https://i.imgur.com/smlDd1e.png)

## Discussion
### MSE Loss or Cross Entropy Loss?
Through the training process, we’ll forward our input value to get the predicted label in the end. According to the setting of loss function, we impose penalty generated by comparing with ground truth y and predicted label y head.<br>
In our implementation in this lab, I use Entropy loss instead of MSE loss, which may accelerate my training performance and force our model to modify parameter based on “more” error input.<br>
Let’s take a look at these two different loss functions and their derivative form.<br>
#### Cross Entropy Loss
![](https://i.imgur.com/9MlmLu7.png)<br><br>
![](https://i.imgur.com/nHtgmJm.png)<br><br>
![](https://i.imgur.com/x7RlzQV.png)<br><br>
#### MSE Loss
![](https://i.imgur.com/HmXnZnn.png)<br><br>
![](https://i.imgur.com/Y5uwGaR.png)<br><br>

Only formula of these two functions is hard to find out which one is suitable for our situation.
The following graph will show a compared curve between these two functions, when y = 0 and y head is plotted from 0 to 1.<br>
![](https://i.imgur.com/tOwxeHd.png)<br>
Therefore, Entropy will impose a larger penalty on a farther error term. Growing exponentially faster when y head is apart from ground truth, which may force model to update more according to the data point far from ground truth.
